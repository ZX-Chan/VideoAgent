
import os
import json
import tempfile
from typing import Tuple, Dict, Any, Optional
from camel.models import ModelFactory
from camel.agents import ChatAgent
from camel.messages import BaseMessage
from utils.wei_utils import account_token
from utils.src.utils import get_json_from_response


def generate_manim_with_agents(
    args,
    raw_content: Dict[str, Any],
    agent_config: Dict[str, Any]
) -> Tuple[str, int, int]:
    """
    ä½¿ç”¨ Agent1 å’Œ Agent2 è‡ªåŠ¨ç”Ÿæˆå®Œæ•´çš„ Manim åŠ¨ç”»ä»£ç 
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        raw_content: è®ºæ–‡çš„åŸå§‹å†…å®¹æ•°æ®
        agent_config: Agent æ¨¡å‹é…ç½®
        
    Returns:
        (manim_code_string, total_input_tokens, total_output_tokens)
    """
    print("ğŸ¤– å¼€å§‹è°ƒç”¨ AI Agents ç”Ÿæˆ Manim åŠ¨ç”»ä»£ç ...")
    
    total_input_tokens = 0
    total_output_tokens = 0
    
    try:
        # Step 1: è°ƒç”¨ Agent1 è¿›è¡ŒåŠ¨ç”»è§„åˆ’
        print("ğŸ“‹ Step 1: è°ƒç”¨ Agent1 (åŠ¨ç”»è§„åˆ’å¸ˆ) åˆ†æè®ºæ–‡å¹¶ç”ŸæˆåŠ¨ç”»è§„åˆ’...")
        planning_json, tokens1_in, tokens1_out = call_agent1_animation_planner(
            raw_content, args, agent_config
        )
        total_input_tokens += tokens1_in
        total_output_tokens += tokens1_out
        print(f"âœ… Agent1 å®Œæˆï¼Œç”Ÿæˆäº† {len(planning_json.get('scene_sequence', []))} ä¸ªåŠ¨ç”»åœºæ™¯")
        
        # Step 2: è°ƒç”¨ Agent2 ç”Ÿæˆ Manim ä»£ç 
        print("ğŸ’» Step 2: è°ƒç”¨ Agent2 (ä»£ç ç”Ÿæˆå™¨) æ ¹æ®è§„åˆ’ç”Ÿæˆ Manim ä»£ç ...")
        manim_code, tokens2_in, tokens2_out = call_agent2_code_generator(
            planning_json, args, agent_config
        )
        total_input_tokens += tokens2_in
        total_output_tokens += tokens2_out
        print("âœ… Agent2 å®Œæˆï¼Œç”Ÿæˆäº†å®Œæ•´çš„ Manim åŠ¨ç”»ä»£ç ")
        
        # Step 3: ä»£ç éªŒè¯
        if validate_manim_code(manim_code):
            print("âœ… ç”Ÿæˆçš„ Manim ä»£ç é€šè¿‡åŸºç¡€éªŒè¯")
            return manim_code, total_input_tokens, total_output_tokens
        else:
            raise ValueError("ç”Ÿæˆçš„ä»£ç æœªé€šè¿‡éªŒè¯")
            
    except Exception as e:
        print(f"âŒ Agent ç”Ÿæˆå¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨é»˜è®¤æ¨¡æ¿ä½œä¸ºé™çº§æ–¹æ¡ˆ...")
        default_code = create_default_manim_script(args.poster_name)
        return default_code, total_input_tokens, total_output_tokens


def call_agent1_animation_planner(
    raw_content: Dict[str, Any],
    args,
    agent_config: Dict[str, Any]
) -> Tuple[Dict[str, Any], int, int]:
    """
    è°ƒç”¨ Agent1 è¿›è¡ŒåŠ¨ç”»è§„åˆ’
    
    Args:
        raw_content: è®ºæ–‡åŸæ–‡å†…å®¹
        args: åŒ…å«poster_nameç­‰å‚æ•°
        agent_config: Agenté…ç½®
        
    Returns:
        (planning_json, input_tokens, output_tokens)
    """
    # åŠ è½½ Agent1 æç¤ºè¯
    agent1_prompt_path = 'utils/prompts/agent1.txt'
    with open(agent1_prompt_path, 'r', encoding='utf-8') as f:
        agent1_system_prompt = f.read()
    
    # åˆ›å»º Agent1 æ¨¡å‹
    model = ModelFactory.create(
        model_platform=agent_config['model_platform'],
        model_type=agent_config['model_type'],
        model_config_dict=agent_config['model_config'],
        url=agent_config.get('url', None)
    )
    
    agent1 = ChatAgent(
        system_message=agent1_system_prompt,
        model=model,
        message_window_size=None,
    )
    
    # æå–è®ºæ–‡åŸæ–‡å†…å®¹
    paper_original_text = extract_paper_original_text(raw_content)
    
    # è·å–ç›®æ ‡å›¾è¡¨ä¿¡æ¯ï¼ˆç”¨äºç”Ÿæˆç®€çŸ­éœ€æ±‚ï¼‰
    target_figure_info = select_target_figure(raw_content, args)
    
    # æ„å»ºç®€çŸ­éœ€æ±‚
    short_requirement = generate_short_requirement(target_figure_info, args.poster_name)
    
    # æŒ‰ç…§æ–°è¦æ±‚æ„å»ºç”¨æˆ·è¾“å…¥ï¼šè®ºæ–‡åŸæ–‡ + ç®€çŸ­éœ€æ±‚
    user_input = f"""
# è®ºæ–‡åŸæ–‡
{paper_original_text}

# åŠ¨ç”»éœ€æ±‚
{short_requirement}

è¯·åŸºäºä»¥ä¸Šè®ºæ–‡åŸæ–‡å†…å®¹å’ŒåŠ¨ç”»éœ€æ±‚ï¼Œç”Ÿæˆè¯¦ç»†çš„åŠ¨ç”»è§„åˆ’JSONã€‚
"""
    
    # è°ƒç”¨ Agent1
    response = agent1.step(user_input)
    input_tokens, output_tokens = account_token(response)
    
    # è§£æå“åº”ä¸º JSON
    planning_json = get_json_from_response(response.msgs[0].content)
    
    if not planning_json:
        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªåŸºç¡€çš„è§„åˆ’ç»“æ„
        planning_json = create_fallback_planning(args.poster_name)
    
    return planning_json, input_tokens, output_tokens


def call_agent2_code_generator(
    planning_json: Dict[str, Any],
    args,
    agent_config: Dict[str, Any]
) -> Tuple[str, int, int]:
    """
    è°ƒç”¨ Agent2 ç”Ÿæˆ Manim ä»£ç 
    
    Args:
        planning_json: Agent1äº§ç”Ÿçš„JSONè§„åˆ’
        args: åŒ…å«poster_nameç­‰å‚æ•°
        agent_config: Agenté…ç½®
        
    Returns:
        (manim_code, input_tokens, output_tokens)
    """
    # åŠ è½½ Agent2 æç¤ºè¯
    agent2_prompt_path = 'utils/prompts/agent2.txt'
    with open(agent2_prompt_path, 'r', encoding='utf-8') as f:
        agent2_system_prompt = f.read()
    
    # åˆ›å»º Agent2 æ¨¡å‹
    model = ModelFactory.create(
        model_platform=agent_config['model_platform'],
        model_type=agent_config['model_type'],
        model_config_dict=agent_config['model_config'],
        url=agent_config.get('url', None)
    )
    
    agent2 = ChatAgent(
        system_message=agent2_system_prompt,
        model=model,
        message_window_size=None,
    )
    
    # è·å–ç›¸åº”çš„figureå›¾ç‰‡ä¿¡æ¯
    figure_image_info = get_target_figure_image(args)
    
    # æ„å»ºç”¨æˆ·è¾“å…¥ï¼šAgent1äº§ç”Ÿçš„JSON + ç›¸åº”figureå›¾ç‰‡
    scene_class_name = f"{sanitize_class_name(args.poster_name)}Animation"
    
    user_input = f"""
# Agent1äº§ç”Ÿçš„åŠ¨ç”»è§„åˆ’JSON
{json.dumps(planning_json, indent=2, ensure_ascii=False)}

# ç›¸åº”figureå›¾ç‰‡ä¿¡æ¯
{figure_image_info}

# ä»£ç ç”Ÿæˆè¦æ±‚
- åœºæ™¯ç±»å: {scene_class_name}
- ç»§æ‰¿è‡ª Scene ç±»
- åŒ…å«å®Œæ•´çš„ construct æ–¹æ³•
- ä½¿ç”¨æ ‡å‡† Manim è¯­æ³•
- ä»£ç å¿…é¡»å¯ä»¥ç›´æ¥æ‰§è¡Œ
- å‚è€ƒå›¾ç‰‡ä¿¡æ¯æ¥å‡†ç¡®è¿˜åŸè§†è§‰æ•ˆæœ

# Manim æ ‡å‡†å½¢çŠ¶ç±»ï¼ˆåªä½¿ç”¨è¿™äº›ï¼‰
- Rectangle: çŸ©å½¢
- Circle: åœ†å½¢
- Square: æ­£æ–¹å½¢
- RegularPolygon: æ­£å¤šè¾¹å½¢ï¼ˆå¯ç”¨äºä¸‰è§’å½¢ã€å…­è§’å½¢ç­‰ï¼‰
- Ellipse: æ¤­åœ†
- Line: ç›´çº¿
- Arrow: ç®­å¤´
- Text: æ–‡æœ¬
- Dot: ç‚¹

# æ³¨æ„ï¼šç¦æ­¢ä½¿ç”¨ Diamond, Triangle, Hexagon ç­‰ä¸å­˜åœ¨çš„ç±»

è¯·æ ¹æ®JSONè§„åˆ’å’Œå›¾ç‰‡ä¿¡æ¯ç”Ÿæˆå®Œæ•´çš„Pythonä»£ç æ–‡ä»¶å†…å®¹ã€‚
"""
    
    # è°ƒç”¨ Agent2
    response = agent2.step(user_input)
    input_tokens, output_tokens = account_token(response)
    
    # æå–ä»£ç å†…å®¹
    manim_code = extract_python_code(response.msgs[0].content)
    
    if not manim_code or len(manim_code) < 100:
        # å¦‚æœä»£ç æå–å¤±è´¥æˆ–å¤ªçŸ­ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
        manim_code = create_default_manim_script(args.poster_name)
    
    return manim_code, input_tokens, output_tokens


def extract_paper_original_text(raw_content: Dict[str, Any]) -> str:
    """æå–è®ºæ–‡åŸæ–‡å†…å®¹"""
    try:
        sections = raw_content.get('sections', [])
        
        # ç®€æ´åœ°æå–æ‰€æœ‰ç« èŠ‚çš„åŸæ–‡å†…å®¹
        original_text = ""
        
        for section in sections:
            title = section.get('title', '')
            content = section.get('content', '')
            
            original_text += f"\n## {title}\n"
            original_text += f"{content}\n"
        
        return original_text.strip()
        
    except Exception as e:
        return f"è®ºæ–‡åŸæ–‡æå–é”™è¯¯: {e}"


def generate_short_requirement(target_figure_info: str, poster_name: str) -> str:
    """æ ¹æ®ç›®æ ‡å›¾è¡¨ä¿¡æ¯ç”Ÿæˆç®€çŸ­çš„åŠ¨ç”»éœ€æ±‚"""
    try:
        # ä»å›¾è¡¨ä¿¡æ¯ä¸­æå–å…³é”®å†…å®¹æ¥ç”Ÿæˆéœ€æ±‚
        if "figure" in target_figure_info.lower() or "å›¾" in target_figure_info:
            # å¦‚æœæœ‰å…·ä½“çš„å›¾è¡¨ä¿¡æ¯
            if "architecture" in target_figure_info.lower() or "æ¡†æ¶" in target_figure_info:
                requirement = f"æ ¹æ®è®ºæ–‡ä¸­çš„ç³»ç»Ÿæ¶æ„å›¾ï¼Œç»™å‡ºè¯¦ç»†çš„åŠ¨æ€æ¼”ç¤ºï¼Œè¦è®²æ¸…æ¥šæ•´ä¸ªç³»ç»Ÿçš„å·¥ä½œæµç¨‹å’Œå„ç»„ä»¶çš„äº¤äº’å…³ç³»"
            elif "flow" in target_figure_info.lower() or "æµç¨‹" in target_figure_info:
                requirement = f"æ ¹æ®è®ºæ–‡ä¸­çš„ç®—æ³•æµç¨‹å›¾ï¼Œç»™å‡ºè¯¦ç»†çš„åŠ¨æ€æ¼”ç¤ºï¼Œè¦è®²æ¸…æ¥šç®—æ³•çš„æ‰§è¡Œæ­¥éª¤å’Œæ•°æ®æµå‘"
            elif "model" in target_figure_info.lower() or "æ¨¡å‹" in target_figure_info:
                requirement = f"æ ¹æ®è®ºæ–‡ä¸­çš„æ¨¡å‹ç»“æ„å›¾ï¼Œç»™å‡ºè¯¦ç»†çš„åŠ¨æ€æ¼”ç¤ºï¼Œè¦è®²æ¸…æ¥šæ¨¡å‹çš„ç»„æˆå’Œå·¥ä½œåŸç†"
            else:
                requirement = f"æ ¹æ®è®ºæ–‡ä¸­çš„æ ¸å¿ƒå›¾è¡¨ï¼Œç»™å‡ºè¯¦ç»†çš„åŠ¨æ€æ¼”ç¤ºï¼Œè¦è®²æ¸…æ¥šå›¾è¡¨æ‰€è¡¨è¾¾çš„æ ¸å¿ƒæ€æƒ³å’ŒæŠ€æœ¯æ–¹æ³•"
        else:
            # å¦‚æœæ²¡æœ‰å…·ä½“å›¾è¡¨ä¿¡æ¯ï¼Œç”Ÿæˆé€šç”¨éœ€æ±‚
            requirement = f"æ ¹æ®è®ºæ–‡ã€Š{poster_name}ã€‹çš„æ ¸å¿ƒå†…å®¹ï¼Œåˆ›å»ºä¸€ä¸ªåŠ¨æ€æ¼”ç¤ºåŠ¨ç”»ï¼Œè¦è®²æ¸…æ¥šè®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’ŒæŠ€æœ¯åˆ›æ–°ç‚¹"
        
        return requirement
        
    except Exception as e:
        return f"æ ¹æ®è®ºæ–‡ã€Š{poster_name}ã€‹çš„æ ¸å¿ƒå†…å®¹ï¼Œåˆ›å»ºä¸€ä¸ªåŠ¨æ€æ¼”ç¤ºåŠ¨ç”»ï¼Œè¦è®²æ¸…æ¥šè®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’ŒæŠ€æœ¯åˆ›æ–°ç‚¹"


def get_target_figure_image(args) -> str:
    """è·å–ç›¸åº”figureå›¾ç‰‡çš„ä¿¡æ¯"""
    try:
        from utils.path_utils import get_paper_name_from_path, load_json_file
        
        paper_name = get_paper_name_from_path(args.poster_path)
        
        # åŠ è½½å›¾ç‰‡ä¿¡æ¯
        try:
            images = load_json_file('images_and_tables', paper_name, f'{args.poster_name}_images.json', 'qwen-2.5-vl-7b', '4o')
            
            if images:
                # é€‰æ‹©ç¬¬ä¸€ä¸ªå›¾ç‰‡ä½œä¸ºç›®æ ‡å›¾ç‰‡
                first_image_id = list(images.keys())[0]
                first_image = images[first_image_id]
                
                image_info = f"""
ç›®æ ‡Figureå›¾ç‰‡ä¿¡æ¯:
- å›¾ç‰‡ID: {first_image_id}
- å›¾ç‰‡è·¯å¾„: {first_image.get('image_path', '')}
- å›¾ç‰‡æ ‡é¢˜: {first_image.get('caption', '')}
- å›¾ç‰‡æè¿°: {first_image.get('description', '')}

å›¾ç‰‡è¯¦ç»†ä¿¡æ¯:
{json.dumps(first_image, indent=2, ensure_ascii=False)}

æ³¨æ„: è¯·æ ¹æ®è¿™ä¸ªå›¾ç‰‡çš„å®é™…å†…å®¹å’Œç»“æ„æ¥è®¾è®¡ManimåŠ¨ç”»ï¼Œç¡®ä¿åŠ¨ç”»èƒ½å¤Ÿå‡†ç¡®åæ˜ å›¾ç‰‡ä¸­çš„å…³é”®å…ƒç´ å’Œå¸ƒå±€ã€‚
"""
                return image_info
            else:
                return "æœªæ‰¾åˆ°ç›¸åº”çš„figureå›¾ç‰‡ä¿¡æ¯ï¼Œè¯·æ ¹æ®JSONè§„åˆ’å†…å®¹ç”Ÿæˆé€šç”¨çš„åŠ¨ç”»ä»£ç ã€‚"
                
        except Exception as e:
            return f"å›¾ç‰‡ä¿¡æ¯åŠ è½½å¤±è´¥: {e}ï¼Œè¯·æ ¹æ®JSONè§„åˆ’å†…å®¹ç”Ÿæˆé€šç”¨çš„åŠ¨ç”»ä»£ç ã€‚"
            
    except Exception as e:
        return f"è·å–å›¾ç‰‡ä¿¡æ¯æ—¶å‡ºé”™: {e}ï¼Œè¯·æ ¹æ®JSONè§„åˆ’å†…å®¹ç”Ÿæˆé€šç”¨çš„åŠ¨ç”»ä»£ç ã€‚"


def classify_section_type(title: str) -> str:
    """æ ¹æ®æ ‡é¢˜åˆ†ç±»ç« èŠ‚ç±»å‹"""
    title_lower = title.lower()
    
    if any(keyword in title_lower for keyword in ['abstract', 'æ‘˜è¦']):
        return 'Abstract'
    elif any(keyword in title_lower for keyword in ['introduction', 'å¼•è¨€', 'ç»¼è¿°']):
        return 'Introduction'
    elif any(keyword in title_lower for keyword in ['method', 'approach', 'æ–¹æ³•', 'ç®—æ³•']):
        return 'Method'
    elif any(keyword in title_lower for keyword in ['experiment', 'result', 'å®éªŒ', 'ç»“æœ']):
        return 'Experiment'
    elif any(keyword in title_lower for keyword in ['conclusion', 'ç»“è®º', 'æ€»ç»“']):
        return 'Conclusion'
    elif any(keyword in title_lower for keyword in ['related', 'ç›¸å…³å·¥ä½œ']):
        return 'Related Work'
    elif any(keyword in title_lower for keyword in ['implementation', 'å®ç°']):
        return 'Implementation'
    else:
        return 'Other'


def select_target_figure(raw_content: Dict[str, Any], args) -> str:
    """æ™ºèƒ½é€‰æ‹©ç”¨äºåŠ¨ç”»çš„ç›®æ ‡å›¾è¡¨å¹¶æä¾›è¯¦ç»†ä¿¡æ¯"""
    try:
        # è·å–è®ºæ–‡åç§°ç”¨äºæ‰¾åˆ°å›¾è¡¨æ–‡ä»¶
        from utils.path_utils import get_paper_name_from_path, load_json_file
        
        paper_name = get_paper_name_from_path(args.poster_path)
        
        # å°è¯•åŠ è½½å›¾è¡¨å’Œè¡¨æ ¼ä¿¡æ¯
        try:
            images = load_json_file('images_and_tables', paper_name, f'{args.poster_name}_images.json', 'qwen-2.5-vl-7b', '4o')
            tables = load_json_file('images_and_tables', paper_name, f'{args.poster_name}_tables.json', 'qwen-2.5-vl-7b', '4o')
        except:
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¿”å›é€šç”¨æè¿°
            return "ç›®æ ‡å›¾è¡¨: è®ºæ–‡ä¸­çš„æ ¸å¿ƒæ–¹æ³•æ¶æ„å›¾æˆ–ç®—æ³•æµç¨‹å›¾"
        
        # åˆ†æå›¾è¡¨å†…å®¹å¹¶é€‰æ‹©æœ€é€‚åˆçš„
        figure_analysis = {
            'total_images': len(images) if images else 0,
            'total_tables': len(tables) if tables else 0,
            'selected_figures': []
        }
        
        # ä¼˜å…ˆé€‰æ‹©æ¶æ„å›¾ã€æµç¨‹å›¾ç­‰é€‚åˆåŠ¨ç”»çš„å›¾ç‰‡
        priority_keywords = ['architecture', 'framework', 'pipeline', 'flow', 'diagram', 'model', 'system']
        
        if images:
            for img_id, img_info in images.items():
                img_path = img_info.get('image_path', '')
                caption = img_info.get('caption', '').lower()
                
                # è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°
                priority_score = 0
                for keyword in priority_keywords:
                    if keyword in caption:
                        priority_score += 1
                
                figure_info = {
                    'id': img_id,
                    'path': img_path,
                    'caption': img_info.get('caption', ''),
                    'priority_score': priority_score,
                    'type': 'image'
                }
                figure_analysis['selected_figures'].append(figure_info)
        
        # ä¹Ÿè€ƒè™‘è¡¨æ ¼ï¼Œå°¤å…¶æ˜¯ç»“æœè¡¨
        if tables:
            for table_id, table_info in tables.items():
                table_path = table_info.get('table_path', '')
                caption = table_info.get('caption', '').lower()
                
                # è¡¨æ ¼çš„ä¼˜å…ˆçº§è¾ƒä½ï¼Œä½†ç»“æœè¡¨å¯ä»¥è€ƒè™‘
                priority_score = 0
                if any(keyword in caption for keyword in ['result', 'performance', 'comparison']):
                    priority_score = 0.5
                
                figure_info = {
                    'id': table_id,
                    'path': table_path,
                    'caption': table_info.get('caption', ''),
                    'priority_score': priority_score,
                    'type': 'table'
                }
                figure_analysis['selected_figures'].append(figure_info)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        figure_analysis['selected_figures'].sort(key=lambda x: x['priority_score'], reverse=True)
        
        # ç”Ÿæˆè¯¦ç»†çš„å›¾è¡¨ä¿¡æ¯æè¿°
        if figure_analysis['selected_figures']:
            top_figure = figure_analysis['selected_figures'][0]
            
            figure_description = f"""
å›¾è¡¨åˆ†æç»“æœ:
- æ€»å›¾ç‰‡æ•°: {figure_analysis['total_images']}
- æ€»è¡¨æ ¼æ•°: {figure_analysis['total_tables']}

æ¨èç›®æ ‡å›¾è¡¨:
- ID: {top_figure['id']}
- ç±»å‹: {top_figure['type']}
- è·¯å¾„: {top_figure['path']}
- æ ‡é¢˜: {top_figure['caption']}
- ä¼˜å…ˆçº§åˆ†æ•°: {top_figure['priority_score']}

å…¶ä»–å¯ç”¨å›¾è¡¨:
"""
            
            for i, fig in enumerate(figure_analysis['selected_figures'][1:5], 1):  # æ˜¾ç¤ºå‰5ä¸ª
                figure_description += f"- {i}. {fig['type']}: {fig['caption'][:100]}...\n"
            
            return figure_description
        else:
            return "ç›®æ ‡å›¾è¡¨: è®ºæ–‡ä¸­çš„æ ¸å¿ƒæ–¹æ³•æ¶æ„å›¾æˆ–ç®—æ³•æµç¨‹å›¾ï¼ˆæœªæ‰¾åˆ°å…·ä½“å›¾è¡¨æ–‡ä»¶ï¼‰"
        
    except Exception as e:
        return f"å›¾è¡¨åˆ†æé”™è¯¯: {e}\né»˜è®¤ç›®æ ‡: è®ºæ–‡ä¸­çš„æ ¸å¿ƒæ–¹æ³•æ¶æ„å›¾æˆ–ç®—æ³•æµç¨‹å›¾"


def sanitize_class_name(poster_name: str) -> str:
    """æ¸…ç†ç±»åï¼Œç¡®ä¿ç¬¦åˆPythonå‘½åè§„èŒƒ"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯å’Œæ•°å­—
    import re
    clean_name = re.sub(r'[^a-zA-Z0-9]', '', poster_name)
    # ç¡®ä¿ä»¥å­—æ¯å¼€å¤´
    if clean_name and clean_name[0].isdigit():
        clean_name = 'Paper' + clean_name
    elif not clean_name:
        clean_name = 'PaperAnimation'
    return clean_name


def extract_python_code(response_text: str) -> str:
    """ä»å“åº”ä¸­æå–Pythonä»£ç """
    import re
    
    # å°è¯•æå–ä»£ç å—
    code_patterns = [
        r'```python\n(.*?)\n```',
        r'```\n(.*?)\n```',
        r'from manim import \*(.*?)(?=\n\n|\Z)',
    ]
    
    for pattern in code_patterns:
        matches = re.findall(pattern, response_text, re.DOTALL)
        if matches:
            code = matches[0].strip()
            if 'from manim import' in code and 'class' in code:
                return code
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œè¿”å›æ•´ä¸ªå“åº”ï¼ˆå¯èƒ½æ˜¯çº¯ä»£ç ï¼‰
    if 'from manim import' in response_text and 'class' in response_text:
        return response_text.strip()
    
    return ""


def validate_manim_code(code: str) -> bool:
    """éªŒè¯ç”Ÿæˆçš„Manimä»£ç åŸºæœ¬è¯­æ³•å’ŒAPIæ­£ç¡®æ€§"""
    try:
        # åŸºæœ¬æ£€æŸ¥
        required_elements = [
            'from manim import',
            'class',
            'Scene',
            'def construct',
        ]
        
        for element in required_elements:
            if element not in code:
                print(f"âš ï¸ ä»£ç éªŒè¯å¤±è´¥: ç¼ºå°‘ '{element}'")
                return False
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ä¸å­˜åœ¨çš„Manimç±»
        invalid_manim_classes = [
            'Diamond',  # ä¸å­˜åœ¨çš„å½¢çŠ¶
            'Triangle',  # åº”è¯¥ä½¿ç”¨ Polygon
            'Hexagon',   # åº”è¯¥ä½¿ç”¨ RegularPolygon
            'Pentagon',  # åº”è¯¥ä½¿ç”¨ RegularPolygon
        ]
        
        for invalid_class in invalid_manim_classes:
            if invalid_class in code:
                print(f"âš ï¸ ä»£ç éªŒè¯å¤±è´¥: ä½¿ç”¨äº†ä¸å­˜åœ¨çš„Manimç±» '{invalid_class}'")
                print(f"å»ºè®®ä½¿ç”¨: RegularPolygon æˆ–å…¶ä»–æ ‡å‡†å½¢çŠ¶")
                return False
        
        # å°è¯•ç¼–è¯‘æ£€æŸ¥
        compile(code, '<string>', 'exec')
        return True
        
    except SyntaxError as e:
        print(f"âš ï¸ ä»£ç è¯­æ³•é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âš ï¸ ä»£ç éªŒè¯å¤±è´¥: {e}")
        return False


def create_fallback_planning(poster_name: str) -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤çš„åŠ¨ç”»è§„åˆ’ç»“æ„"""
    return {
        "paper_analysis": {
            "research_domain": "academic research",
            "core_method": "research methodology",
            "figure_purpose": "illustrate key concepts",
            "technical_complexity": "medium"
        },
        "animation_design": {
            "total_duration": "60",
            "scene_sequence": [
                {
                    "scene_id": 1,
                    "name": "Introduction",
                    "duration": "20",
                    "content_focus": "Paper overview",
                    "animation_elements": [
                        {
                            "target": "title",
                            "action": "Write",
                            "timing": "0",
                            "duration": "3"
                        }
                    ]
                }
            ]
        }
    }


def create_default_manim_script(poster_name: str) -> str:
    """åˆ›å»ºé»˜è®¤çš„ManimåŠ¨ç”»è„šæœ¬"""
    class_name = sanitize_class_name(poster_name)
    
    return f'''from manim import *
import numpy as np

class {class_name}Animation(Scene):
    def construct(self):
        # åœºæ™¯é…ç½®
        self.camera.background_color = WHITE
        
        # åˆ›å»ºæ ‡é¢˜
        title = Text("{poster_name}", font_size=48, color=BLACK)
        title.to_edge(UP)
        
        # åˆ›å»ºä¸»è¦å†…å®¹
        content = VGroup(
            Text("å­¦æœ¯è®ºæ–‡åŠ¨ç”»æ¼”ç¤º", font_size=36, color=DARK_BLUE),
            Text("æ ¸å¿ƒæ–¹æ³•ä¸è´¡çŒ®", font_size=24, color=DARK_GRAY),
        ).arrange(DOWN, buff=0.5)
        content.next_to(title, DOWN, buff=1)
        
        # åŠ¨ç”»åºåˆ—
        self.play(Write(title), run_time=2)
        self.wait(1)
        
        self.play(FadeIn(content), run_time=2)
        self.wait(2)
        
        # åˆ›å»ºæ¡†æ¶å›¾
        framework = VGroup(
            Rectangle(width=3, height=2, color=BLUE),
            Text("æ ¸å¿ƒç®—æ³•", font_size=20, color=BLUE)
        )
        framework.next_to(content, DOWN, buff=1)
        
        self.play(DrawBorderThenFill(framework[0]), run_time=1.5)
        self.play(Write(framework[1]), run_time=1)
        self.wait(3)
        
        # ç»“æŸ
        self.play(FadeOut(VGroup(title, content, framework)), run_time=2)
        self.wait(1)
'''


def create_default_manim_script_file(script_path: str, poster_name: str):
    """åˆ›å»ºé»˜è®¤çš„Manimè„šæœ¬æ–‡ä»¶"""
    script_content = create_default_manim_script(poster_name)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(script_path), exist_ok=True)
    
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print(f"âœ… é»˜è®¤Manimè„šæœ¬å·²åˆ›å»º: {script_path}")


def analyze_research_context(raw_content: Dict[str, Any]) -> Dict[str, str]:
    """åˆ†æç ”ç©¶èƒŒæ™¯å’Œé¢†åŸŸ"""
    try:
        sections = raw_content.get('sections', [])
        
        # åˆ†æç ”ç©¶é¢†åŸŸ
        domain_keywords = {
            'deep_learning': ['deep learning', 'neural network', 'cnn', 'rnn', 'transformer'],
            'nlp': ['natural language', 'nlp', 'text', 'language model', 'bert'],
            'computer_vision': ['computer vision', 'image', 'visual', 'object detection'],
            'machine_learning': ['machine learning', 'ml', 'algorithm', 'model'],
            'ai': ['artificial intelligence', 'ai', 'intelligent'],
            'data_science': ['data', 'dataset', 'analysis', 'mining'],
            'robotics': ['robot', 'robotic', 'autonomous'],
            'other': []
        }
        
        all_text = ' '.join([section.get('content', '') for section in sections]).lower()
        
        domain_scores = {}
        for domain, keywords in domain_keywords.items():
            score = sum(all_text.count(keyword) for keyword in keywords)
            domain_scores[domain] = score
        
        # æ‰¾åˆ°æœ€åŒ¹é…çš„é¢†åŸŸ
        primary_domain = max(domain_scores, key=domain_scores.get)
        if domain_scores[primary_domain] == 0:
            primary_domain = 'general_research'
        
        # åˆ†æç ”ç©¶åŠ¨æœºå’Œè´¡çŒ®
        motivation_analysis = ""
        contribution_analysis = ""
        
        for section in sections:
            title = section.get('title', '').lower()
            content = section.get('content', '')
            
            if 'introduction' in title or 'abstract' in title:
                motivation_analysis += f"\n{content[:500]}..."
            elif 'conclusion' in title or 'contribution' in title:
                contribution_analysis += f"\n{content[:500]}..."
        
        return {
            'domain': primary_domain,
            'analysis': f"""
ç ”ç©¶é¢†åŸŸ: {primary_domain}
ç ”ç©¶åŠ¨æœº: {motivation_analysis.strip()}
ä¸»è¦è´¡çŒ®: {contribution_analysis.strip()}
"""
        }
        
    except Exception as e:
        return {
            'domain': 'unknown',
            'analysis': f'ç ”ç©¶èƒŒæ™¯åˆ†æé”™è¯¯: {e}'
        }


def assess_technical_complexity(raw_content: Dict[str, Any]) -> str:
    """è¯„ä¼°æŠ€æœ¯å¤æ‚åº¦"""
    try:
        sections = raw_content.get('sections', [])
        all_text = ' '.join([section.get('content', '') for section in sections]).lower()
        
        # å¤æ‚åº¦æŒ‡æ ‡
        complexity_indicators = {
            'mathematical': ['equation', 'formula', 'theorem', 'proof', 'optimization'],
            'algorithmic': ['algorithm', 'complexity', 'time complexity', 'space complexity'],
            'experimental': ['experiment', 'dataset', 'baseline', 'evaluation'],
            'theoretical': ['theoretical', 'theory', 'framework', 'model']
        }
        
        scores = {}
        for category, keywords in complexity_indicators.items():
            score = sum(all_text.count(keyword) for keyword in keywords)
            scores[category] = score
        
        total_score = sum(scores.values())
        
        if total_score > 20:
            return 'very_complex'
        elif total_score > 10:
            return 'complex'
        elif total_score > 5:
            return 'medium'
        else:
            return 'simple'
            
    except Exception as e:
        return 'unknown'